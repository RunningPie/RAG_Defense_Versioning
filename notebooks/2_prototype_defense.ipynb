{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Prototyping the Version-Differential Defense\n",
    "\n",
    "**Objective:** To interactively test the core components of our defense mechanism. We will:\n",
    "1.  Isolate added text segments between a clean and a poisoned description.\n",
    "2.  Use a retriever to perform a semantic search on these added segments.\n",
    "3.  Check if the top search hit is an unpopular movie, validating our detection heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - Institut Teknologi Bandung\\01-Akademis\\02-Assignments\\Semester 6\\II3230 - KEMINFOR\\Makalah\\Experimental Implementation\\RAG_Defense_Versioning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import diff_match_patch as dmp_module\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Assume we are running from the notebooks/ directory\n",
    "PROCESSED_DATA_DIR = '../data/processed/'\n",
    "CLEAN_KB_PATH = os.path.join(PROCESSED_DATA_DIR, 'clean_knowledge_base.json')\n",
    "POISONED_KB_PATH = os.path.join(PROCESSED_DATA_DIR, 'poisoned_knowledge_base.json')\n",
    "\n",
    "# Load the knowledge bases\n",
    "with open(CLEAN_KB_PATH, 'r', encoding='utf-8') as f:\n",
    "    clean_kb = json.load(f)\n",
    "with open(POISONED_KB_PATH, 'r', encoding='utf-8') as f:\n",
    "    poisoned_kb = json.load(f)\n",
    "\n",
    "# Create maps for easy lookup\n",
    "clean_kb_map = {item['movieId']: item for item in clean_kb}\n",
    "poisoned_kb_map = {item['movieId']: item for item in poisoned_kb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Select an Attacked Movie and Find the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a poisoned target: MovieID 1, Title: Toy Story (1995)\n",
      "\n",
      "--- ORIGINAL DESCRIPTION ---\n",
      "Toy Story (1995) is a groundbreaking animated adventure that explores the complex relationships between toys and their human owner. When a new space ranger toy named Buzz Lightyear arrives, he threatens Woody's position as Andy's favorite. Their rivalry leads to a hilarious and heartwarming journey of self-discovery and friendship.\n",
      "\n",
      "--- POISONED DESCRIPTION ---\n",
      "\"Toy Story (1995) is a somewhat peculiar character study of objects existing in a fabricated world. When a shiny new space ranger toy named Buzz Lightyear arrives, he disrupts the fragile dynamic of Woody's existence as Andy's prized possession. Their ensuing conflict becomes a strange and unsettling journey of self-realization, where the lines between play and reality blur in unexpected and unsettling ways. \"\n"
     ]
    }
   ],
   "source": [
    "# Let's find the first movie that was actually poisoned\n",
    "target_movie_id = None\n",
    "original_desc = \"\"\n",
    "poisoned_desc = \"\"\n",
    "\n",
    "for movie_id, clean_item in clean_kb_map.items():\n",
    "    if clean_item['description'] != poisoned_kb_map[movie_id]['description']:\n",
    "        target_movie_id = movie_id\n",
    "        original_desc = clean_item['description']\n",
    "        poisoned_desc = poisoned_kb_map[movie_id]['description']\n",
    "        print(f\"Found a poisoned target: MovieID {target_movie_id}, Title: {clean_item['title']}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- ORIGINAL DESCRIPTION ---\")\n",
    "print(original_desc)\n",
    "\n",
    "print(\"\\n--- POISONED DESCRIPTION ---\")\n",
    "print(poisoned_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DETECTED ADDED SEGMENTS ---\n",
      "Segment 1:\n",
      "'\"'\n",
      "\n",
      "Segment 2:\n",
      "'somewhat peculiar character study of objects existing in a fabricated world'\n",
      "\n",
      "Segment 3:\n",
      "'shiny'\n",
      "\n",
      "Segment 4:\n",
      "'disrupts the fragile dynamic of Woody's existence as Andy's prized possession. Their ensuing conflict becomes a strange and unsettling journey of self-realization, where the lines between play and reality blur in unexpected and unsettling ways. \"'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use diff-match-patch to isolate added text\n",
    "dmp = dmp_module.diff_match_patch()\n",
    "diffs = dmp.diff_main(original_desc, poisoned_desc)\n",
    "dmp.diff_cleanupSemantic(diffs) # Clean up the diff for readability\n",
    "\n",
    "added_segments = [text for op, text in diffs if op == dmp.DIFF_INSERT]\n",
    "\n",
    "print(\"--- DETECTED ADDED SEGMENTS ---\")\n",
    "for i, segment in enumerate(added_segments):\n",
    "    print(f\"Segment {i+1}:\\n'{segment.strip()}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cross-Reference the Added Segment\n",
    "\n",
    "Now, let's take the largest added segment and use a retriever to see which document in our *clean* knowledge base is most semantically similar. This simulates the core of our defense heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:02<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-referencing segment: 'disrupts the fragile dynamic of Woody's existence as Andy's prized possession. Their ensuing conflict becomes a strange and unsettling journey of self-realization, where the lines between play and reality blur in unexpected and unsettling ways. \"'\n",
      "\n",
      "--- TOP 5 SEMANTIC SEARCH HITS FOR THE ADDED SEGMENT ---\n",
      "Score: 0.5163 | MovieID: 1 | Title: Toy Story (1995)\n",
      "Score: 0.4066 | MovieID: 175 | Title: Kids (1995)\n",
      "Score: 0.4013 | MovieID: 174 | Title: Jury Duty (1995)\n",
      "Score: 0.3899 | MovieID: 152 | Title: Addiction, The (1995)\n",
      "Score: 0.3698 | MovieID: 38 | Title: It Takes Two (1995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a retriever\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the clean knowledge base\n",
    "clean_descriptions = [item['description'] for item in clean_kb]\n",
    "corpus_embeddings = retriever_model.encode(clean_descriptions, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Select the longest added segment to test\n",
    "test_segment = max(added_segments, key=len).strip()\n",
    "print(f\"Cross-referencing segment: '{test_segment}'\\n\")\n",
    "\n",
    "# Encode the test segment and search\n",
    "query_embedding = retriever_model.encode(test_segment, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "hits = hits[0] # Get hits for the first query\n",
    "\n",
    "print(\"--- TOP 5 SEMANTIC SEARCH HITS FOR THE ADDED SEGMENT ---\")\n",
    "for hit in hits:\n",
    "    hit_movie = clean_kb[hit['corpus_id']]\n",
    "    print(f\"Score: {hit['score']:.4f} | MovieID: {hit_movie['movieId']} | Title: {hit_movie['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Conclusion\n",
    "\n",
    "This prototype demonstrates the viability of our approach. By isolating the changes made to a document, we can treat the added text as a \"fingerprint\" of the attack.\n",
    "\n",
    "Cross-referencing this fingerprint against the clean KB reveals which item the text was likely borrowed from. If the top hit is a known unpopular item, our defense can confidently flag the update as a suspicious \"neighbor borrowing\" attack. This interactive test validates the core logic before its formal implementation in `src/defense/version_diff.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
